---
name: Test Azure OpenAI Responses API

on:
  workflow_dispatch:  # Allows manual triggering of the workflow

jobs:
  test-responses-api:
    runs-on: ubuntu-latest
    environment: responses  # Use the 'responses' environment for secrets

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test Azure OpenAI Responses API
        env:
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_V1_API_ENDPOINT: ${{ secrets.AZURE_OPENAI_V1_API_ENDPOINT }}
          AZURE_OPENAI_API_MODEL: ${{ secrets.AZURE_OPENAI_API_MODEL }}
        run: |
          echo "Testing responses-basic-aoai-v1.py script..."

          # Verify required environment variables are set
          if [ -z "$AZURE_OPENAI_API_KEY" ] || [ -z "$AZURE_OPENAI_V1_API_ENDPOINT" ] || [ -z "$AZURE_OPENAI_API_MODEL" ]; then
            echo "âŒ Error: Required environment variables are not set"
            echo "AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:+set}"
            echo "AZURE_OPENAI_V1_API_ENDPOINT: ${AZURE_OPENAI_V1_API_ENDPOINT:+set}"
            echo "AZURE_OPENAI_API_MODEL: ${AZURE_OPENAI_API_MODEL:+set}"
            exit 1
          fi

          # Verify jq is available
          if ! command -v jq > /dev/null; then
            echo "âŒ Error: jq is not available"
            exit 1
          fi

          echo "âœ… Environment check passed"

          # Create test results directory
          mkdir -p test-results

          # Get current timestamp
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "Test run timestamp: $timestamp"

          # Run the script and capture output
          echo "Running Python script..."
          python responses-basic-aoai-v1.py > output.txt 2>&1
          exit_code=$?

          # Initialize test result variables
          pass_fail="PASS"
          error_code=""
          output_text=""

          # Check if script executed successfully
          if [ $exit_code -eq 0 ]; then
            echo "âœ… Script executed successfully"

            # Check if output was generated and capture it
            if [ -s output.txt ]; then
              output_text=$(cat output.txt)
              echo "âœ… Script produced output:"
              echo "$output_text"

              # Test whether response.output_text contains a valid string
              # Valid means: non-empty, no error indicators, and actual content
              if [ -n "$output_text" ] && ! echo "$output_text" | grep -qi "error\|exception\|traceback\|failed\|none\|null"; then
                echo "âœ… Output contains valid string content"
                pass_fail="PASS"
              else
                echo "âŒ Output does not contain valid string content"
                pass_fail="FAIL"
                error_code="INVALID_OUTPUT"
              fi
            else
              echo "âŒ Script produced no output"
              pass_fail="FAIL"
              error_code="NO_OUTPUT"
            fi
          else
            echo "âŒ Script failed with exit code: $exit_code"
            echo "Error output:"
            cat output.txt
            output_text=$(cat output.txt)
            pass_fail="FAIL"
            error_code="SCRIPT_ERROR_$exit_code"
          fi

          # Create test results JSON (using jq for proper JSON formatting)
          jq -n \
            --arg timestamp "$timestamp" \
            --arg output "$output_text" \
            --arg pass_fail "$pass_fail" \
            --arg error_code "$error_code" \
            '{
              test_last_run_date: $timestamp,
              output: $output,
              pass_fail: $pass_fail,
              error_code: $error_code
            }' > test-results/test-results.json

          # Also create a human-readable summary
          echo "Azure OpenAI Responses API Test Results" > test-results/test-summary.txt
          echo "========================================" >> test-results/test-summary.txt
          echo "Test Run Date: $timestamp" >> test-results/test-summary.txt
          echo "Result: $pass_fail" >> test-results/test-summary.txt
          echo "Error Code: $error_code" >> test-results/test-summary.txt
          echo "" >> test-results/test-summary.txt
          echo "Output:" >> test-results/test-summary.txt
          echo "$output_text" >> test-results/test-summary.txt

          # Display final results
          echo "=== Test Results ==="
          echo "Timestamp: $timestamp"
          echo "Pass/Fail: $pass_fail"
          echo "Error Code: $error_code"
          echo "Output: $output_text"

          # Exit with error if test failed
          if [ "$pass_fail" = "FAIL" ]; then
            echo "âŒ Test failed"
            exit 1
          else
            echo "ğŸ‰ Test completed successfully!"
          fi

      - name: Upload test results artifact
        uses: actions/upload-artifact@v4
        if: always()  # Upload artifact even if the test fails
        with:
          name: azure-openai-test-results
          path: test-results/
          retention-days: 30